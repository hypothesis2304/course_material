{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will review the concepts of transfer learning and semi-supervised learning and try implementing them.  \n",
    "\n",
    "### Transfer Learning : \n",
    "\n",
    "We know that deep learning models are very data hungry. It is not always possible to collect and annotate millions of data for the task of our interest. Instead what we would like to do is transfer learning. \n",
    "\n",
    "Transfer learning is a phenomena where we use the existing ConvNet model trained on very large dataset as our fixed feature extractor or as a initialization. We then tune this model to our task of concern. The major transfer learning scenarios are as follows:\n",
    "\n",
    "- #### ConvNet as fixed feature extractor : \n",
    "\n",
    "We take an existing Convolutional Neural Network trained on ImageNet(contains 1.2 million images with 1000 categories) dataset and remove the fully connected layers of the pretrained model. We then freeze the layers(don't update the weights) of the ConvNet. Finally, we add our custom fully connected layers after the feature extractor to train the model. In this model, note that the feature extractor(pretrained model) is frozen implies during training only our newly defined fully connected layers are updated and the feature extractor values are fixed.\n",
    "\n",
    "- #### Fine-tuning the ConvNet : \n",
    "\n",
    "The other strategy is to train the model as a whole. In finetuning, we not only replace and retrain the added layers with the new dataset but also we finetune the whole feature extractor with the new dataset. It means, the pretrained model is no more a fixed feature extractor, it is also adapted to the new dataset by updating the weights. Intuitively, it is better to assume the pretrained model as a good initialization rather than a fixed feature extractor. Depending on the size of available dataset, we either fine-tune all the layers of ConvNet or only some of the layer.\n",
    "> - If the datset is too small(<1000 samples) finetuning the whole model increases the chances of overfitting. <br>\n",
    "> - A medium sized datset, finetuning the whole model can be done but regularizing the model carefully will prevent it from overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will implement transfer learning by taking a pretrained ConvNet and using it as a fixed feature extractor. We import \"Resnet-18\" model from the PyTorch models package and use it as a feature extractor. \n",
    "We freeze all the layers of Resnet-18 and add a fully-connected layer on the top of it. As discussed above, we train the model (frozen feature extractor + new fc layer) with the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "# import dataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "# from data_list import ImageList\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc.weight True\n",
      "fc.bias True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# define a pretrained model \"Resnet-18\" from the models package\n",
    "model = models.resnet18(pretrained=True)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'fc' in name:\n",
    "        print(name, param.requires_grad)\n",
    "    \n",
    "print(model.layer3[0].conv2.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a fully connected layer compatible with our dataset and replace it with the existing layer. We know that to initialize a fc-layer we requires the parameters: input size and output size.\n",
    "\n",
    "- Input dim is the output dim of the flattened layer of the feature extractor.\n",
    "- Output size is the number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output dimension of flatten layer is same as input dimension of fc layer. So, getting the input dimes\n",
    "inpSize = model.fc.in_features\n",
    "output_dim = None\n",
    "\n",
    "# define the fully connected layer below.\n",
    "fc_layer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the existing fully connected layer in Resnet-18 with the newly defined fully connected layer. We can access the fully connected layer in model as **model.fc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d5c57ef57fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we have defined the model. But, we want the pretrained model to be a fixed feature extractor. As discussed in the above theory, for the model to be used as a fixed feature extractor we need to freeze all the layers other than fully connected layers in the model.    \n",
    "\n",
    "Freezing the layers is the same as preventing the layers from updating their weights in the optimization step. As discussed in the PyTorch tutorial we can freeze the layers by changing the requires_grad parameter of every layer in the model from True to False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using for loop we iterate through all the parameters of the model.\n",
    "## we need to turn off the requires_grad for all tha layers other than fc layer.\n",
    "## update the if condition, such that fc layers are not considered\n",
    "## update the param.requires_grad such that layers are frozen.\n",
    "\n",
    "for name,param in model.named_paramters():\n",
    "    if _______:\n",
    "        param.requires_grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must define an optimizer for udpating the weights and biases of the model. In this assignment we will use an Adam optimizer. *Take a look at the PyTorch tutorial on how to use the optimizer*. Observe carefully that optimizer expects the model parameters as an input argument. In our assignment, we froze the feature extractor and update only the fully connected layer. To optimize only the trainable parameters we filter out the non-trainable parameters \n",
    "\n",
    "we can use **filter** function and **lambda operator** to filter out all the non-trainable parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the model fully setup, we now define the hyper parameters we use for training the model.\n",
    "epochs = 5\n",
    "learning_rate = None\n",
    "batch_size = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model initialized, we now train the model with our new dataset. We follow the same steps we worked with in the previous assignment to train our model. We also record the losses for every epoch and plot them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Loop\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop \n",
    "# As we don't want the model to track running stats in batch norm, turn off the model in training mode and switch\n",
    "# to evaluation mode.\n",
    "'''\n",
    "CODE HERE\n",
    "'''\n",
    "\n",
    "# With model in evaluation mode, we pass the test images to our model and compute the accuracy.\n",
    "# During Inference, we are sure that we don't compute any backward pass. In order to disable the gradient\n",
    "# calculation and reduce memory consumption, we first call the no_grad function from the PyTorch autograd\n",
    "# package and then pass the test_loader to the model.\n",
    "\n",
    "'''\n",
    "CODE FOR CALLING NO_GRAD FUNCTION\n",
    "'''\n",
    "for idx, (images, labels) in enumerate(test_loader):\n",
    "    '''\n",
    "    TESTING LOOP CODE.\n",
    "    '''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
