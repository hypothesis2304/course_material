{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network\n",
    "\n",
    "Generative Adversarial Networks a.k.a GANs are emerging techniques that model high dimensional distribution of data. They achieve this by training a pair of networks, Generator and Discriminator, in competition with each other. As an analogy we can think of these models as an art forger and the other being an art expert. In GAN literature we term Generator as the forger and the duty of the generator is to produce fake images(forgeries) to deceive the art expert(Discriminator). The Discriminator which receives both the real images and fake images tries to distinguish between them and find the fake images. Both are trained simulataneously and are always in competition with each other. This competition between the Generator and Discriminator drives them to improve their models continuously. The models are trained until the generator produces the fake images that are indistinguishable with the real images.  <br>\n",
    "\n",
    "In this setup, the generator do not have access to the synthetic images whereas the discriminator has access to both the real and fake images. While training the models, the supervisory signal to the discriminator is given by knowing whether the images came from a real image stack or generated image stack whereas the generator in the setup receives the error signal only from the discriminator.  \n",
    "\n",
    "Let us define Discriminator D that takes image as input and produces a number **(0/1)** as output and a Generator G that takes random noise as input and outputs a fake image. In practise, G and D are trained alternately i.e., For a fixed generator G, the discriminator D is trained to classify the training data as real(output a value close to 1) or fake(output a value close to 0). In the next step we freeze the Discriminator assuming it as optimal and we the train the generator G to produce a image(fake) that outputs a value close to 1(real) when passed through the discriminator D. Thus, if the generator is perfectly trained then the discriminator D will be maximally confused by the images generated by G and predicts 0.5 for all the inputs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will try to implement a Generative Adversarial Network on MNIST data and generate images that resemble the digits from the MNIST dataset.\n",
    "\n",
    "To implement a GAN, we basically require 5 components:\n",
    "\n",
    "- Real Dataset (real distribution)\n",
    "- Low dimensional random noise that goes into the generator to produce fake images\n",
    "- Generator that generates fake images\n",
    "- Discriminator that acts as an expert to distinguish real and fake images.\n",
    "- Training loop where the competition occurs and models better themselves\n",
    "\n",
    "\n",
    "Let us implement each of the part and train the overall model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(x):\n",
    "    return 2 * x - 1.0\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GANs we input random noise through the generator to produce fake images which resemble the real distribution. Let us define a function which takes (batchsize, dimension) as input and returns a random noise of requested dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 15])\n"
     ]
    }
   ],
   "source": [
    "## bs - batchsize, dim - input dimension of random noise\n",
    "## return a random noise of requested dimension from a normal distribution. \n",
    "def noise(bs, dim):\n",
    "    return torch.randn((bs, dim))\n",
    "a = noise(8,15)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator architecture:\n",
    "\n",
    "- noise_dim -> 1024\n",
    "- ReLU\n",
    "- 1024 -> 1024\n",
    "- ReLU\n",
    "- 1024 -> 784\n",
    "- TanH \n",
    "- clip image [-1, +1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In this cell we define a generator which takes in random noise as input and outputs a image of size equal\n",
    "## to the images from real distribution\n",
    "\n",
    "## we follow the same style for defining the model as worked in our previous assignments.\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100, out_size=1000):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        ### MODEL HERE ######\n",
    "        ### define a fully connected layer from input_dim -> 1024\n",
    "        self.layer1 = nn.Linear(100, 1024)\n",
    "        self.relu = nn.ReLU()\n",
    "        ### define a ReLU activation function\n",
    "        \n",
    "        ### define a fully connected layer from 1024 -> 1024\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "        ### define a ReLU activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "        ### define a fully connected layer from 1024 -> 1024\n",
    "        self.layer3 = nn.Linear(1024, 784)\n",
    "        ### define a TanH activation function\n",
    "        \n",
    "    def forward(x):\n",
    "        ### Make a forward pass of the model through the layers as described in the \n",
    "        ### generator architecture\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.tanh(x)\n",
    "        x = torch.clamp(x, -1, 1)\n",
    "        \n",
    "        ### clip the individual values of the output vector such that all the values lie between [-1, +1]\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator architecture:\n",
    "\n",
    "- input_size->256\n",
    "- LeakyReLU alpha=0.01\n",
    "- 256e->256\n",
    "- LeakyReLU alpha-=0.01\n",
    "- 256->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similar to the generator, we now define a discriminator which takes in a vector and output a value between \n",
    "## 0 and 1. We define the Discriminator as given in the above architecture.\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        ## define a fully connected layer from from input_dim -> 256\n",
    "        self.layer1 = nn.Linear(784, 256)\n",
    "        \n",
    "        ### define a LeakyReLU activation function with alpha 0.01\n",
    "        self.leaky_relu = nn.leaky_relu(alpha=0.01)\n",
    "        \n",
    "        ### define a fully connected layer from 256 -> 256\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        \n",
    "        ### define a LeakyReLU activation function with alpha 0.01\n",
    "        \n",
    "        ### define a fully connected layer from 256 -> 1\n",
    "        self.layer1 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(x):\n",
    "        ### Make a forward pass of the model through the layers as described in the \n",
    "        ### Discriminator architecture\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLoss(real, fake):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GLoss(fake):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the binary cross-entropy loss function from the PyTorch package.\n",
    "## Binary cross-entropy loss \n",
    "\n",
    "bce_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, we define an optimization strategy to optimize the model weights. For this assignment we will use a\n",
    "## stochastic gradient descent \n",
    "\n",
    "optimizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(iterations):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
